{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HKsoHdPNzek_"
      ],
      "authorship_tag": "ABX9TyM6gKmIZOaJnbldxHLvCj8K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5ddc8342b3a941cdabd560d034312db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "warning",
            "description": "Restart Runtime",
            "disabled": false,
            "icon": "check",
            "layout": "IPY_MODEL_59b07fda402343c8b1de3f335833196f",
            "style": "IPY_MODEL_fa4bdd9c448349aa8fd390c958f612b1",
            "tooltip": "Click me"
          }
        },
        "59b07fda402343c8b1de3f335833196f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa4bdd9c448349aa8fd390c958f612b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiomatricardi/TheRiseOfTheCuriourAI/blob/main/TheRiseOfTheCuriourAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Generation using ü§ótransformers"
      ],
      "metadata": {
        "id": "HKsoHdPNzek_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOURCE:\n",
        "https://github.com/patil-suraj/question_generation\n",
        "<br>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This project is aimed as an open source study on question generation with pre-trained transformers (specifically seq-2-seq models) using straight-forward end-to-end methods without much complicated pipelines. The goal is to provide simplified data processing and training scripts and easy to use pipelines for inference.\n",
        "\n",
        "### Multitask QA-QG\n",
        "For answer aware question generation we usually need 3 models, first which will extract answer like spans, second model will generate question on that answer and third will be a QA model which will take the question and produce an answer, then we can compare the two answers to see if the generated question is correct or not.\n",
        "\n",
        "Having 3 models for single task is lot of complexity, so goal is to create a multi-task model which can do all of these 3 tasks\n",
        "\n",
        "- extract answer like spans\n",
        "- generate question based on the answer\n",
        "- QA\n",
        "\n",
        "T5 model is fine-tuned in multi-task way using task prefixes as described in the paper.\n",
        "\n",
        "<img src=\"https://camo.githubusercontent.com/c96395d16bf1363c4d6472c346ede0dbd7e9acdfec17250dcc0e1775b9c6d1e1/68747470733a2f2f692e6962622e636f2f544253336e73722f74352d73732d322e706e67\" width=600>\n",
        "\n",
        "End-to-End question generation (answer agnostic)\n",
        "In end-to-end question generation the model is aksed to generate questions without providing the answers. This paper discusses these ideas in more detail. Here the T5 model is trained to generate multiple questions simultaneously by just providing the context. The questions are seperated by the <sep> token. Here's how the examples are processed\n",
        "\n",
        "input text: Python is a programming language. Created by Guido van Rossum and first released in 1991.\n",
        "\n",
        "target text: Who created Python ? <sep> When was python released ? <sep>\n",
        "\n",
        "All the training details can be found in this wandb project"
      ],
      "metadata": {
        "id": "E8WfP2ryzedD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Repo"
      ],
      "metadata": {
        "id": "BZDqUImLzFEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install all the required libraries"
      ],
      "metadata": {
        "id": "ekpF7-Fm0Dqz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "29C5LYX8zGBd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install nltk\n",
        "!pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0\n",
        "!pip install langchain\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download question generation pipeline file"
      ],
      "metadata": {
        "id": "Tpdo-7NU1a_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://github.com/patil-suraj/question_generation/raw/master/pipelines.py"
      ],
      "metadata": {
        "id": "lc_rG0lw0jIV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download some text files for examples"
      ],
      "metadata": {
        "id": "STaltknu1iMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://github.com/fabiomatricardi/Abstractive-Extractive/raw/main/BERTexplanation.txt\n",
        "!wget https://github.com/fabiomatricardi/Abstractive-Extractive/raw/main/Text%20Summarization%20with%20NLP-%20TextRank%20vs%20Seq2Seq%20vs%20BART.txt\n",
        "!wget https://github.com/fabiomatricardi/Abstractive-Extractive/raw/main/AutomaticTextSummarization.txt\n",
        "!wget https://github.com/fabiomatricardi/Abstractive-Extractive/raw/main/GPT4VsLima.txt\n",
        "!wget https://github.com/fabiomatricardi/Abstractive-Extractive/raw/main/nlp-basics-abstractive-and-extractive-text-summarization.txt"
      ],
      "metadata": {
        "id": "LiJmqQQg1ZW2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the model weights (torch version)\n",
        "### Download the model valhalla/t5-small-e2e-qg locally and move it to `t5-small-e2e-q`  directory"
      ],
      "metadata": {
        "id": "yesQBum82Hlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://huggingface.co/valhalla/t5-small-e2e-qg/resolve/main/added_tokens.json\n",
        "!wget https://huggingface.co/valhalla/t5-small-e2e-qg/resolve/main/config.json\n",
        "!wget https://huggingface.co/valhalla/t5-small-e2e-qg/resolve/main/pytorch_model.bin\n",
        "!wget https://huggingface.co/valhalla/t5-small-e2e-qg/resolve/main/special_tokens_map.json\n",
        "!wget https://huggingface.co/valhalla/t5-small-e2e-qg/resolve/main/spiece.model\n",
        "!wget https://huggingface.co/valhalla/t5-small-e2e-qg/resolve/main/tokenizer_config.json\n",
        "!wget https://huggingface.co/valhalla/t5-small-e2e-qg/resolve/main/training_args.bin\n",
        "\n",
        "!mkdir t5-small-e2e-qg\n",
        "!mv /content/added_tokens.json /content/t5-small-e2e-qg/added_tokens.json\n",
        "!mv /content/config.json  /content/t5-small-e2e-qg/config.json\n",
        "!mv /content/pytorch_model.bin  /content/t5-small-e2e-qg/pytorch_model.bin\n",
        "!mv /content/special_tokens_map.json  /content/t5-small-e2e-qg/special_tokens_map.json\n",
        "!mv /content/spiece.model  /content/t5-small-e2e-qg/spiece.model\n",
        "!mv /content/tokenizer_config.json  /content/t5-small-e2e-qg/tokenizer_config.json\n",
        "!mv /content/training_args.bin  /content/t5-small-e2e-qg/training_args.bin"
      ],
      "metadata": {
        "id": "Zh4SuYt52IH2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Restart Runtime {display-mode: \"form\"}\n",
        "import ipywidgets as widgets\n",
        "def restart(b):\n",
        "  exit()\n",
        "\n",
        "button2 = widgets.Button(\n",
        "    description='Restart Runtime',\n",
        "    disabled=False,\n",
        "    button_style='warning', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Click me',\n",
        "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        ")\n",
        "button2.on_click(restart)\n",
        "button2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5ddc8342b3a941cdabd560d034312db8",
            "59b07fda402343c8b1de3f335833196f",
            "fa4bdd9c448349aa8fd390c958f612b1"
          ]
        },
        "outputId": "977720f6-2679-4090-8a74-45e8b4cad5fe",
        "id": "r0r7ikjKHqMV"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='warning', description='Restart Runtime', icon='check', style=ButtonStyle(), tooltip='Clic‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ddc8342b3a941cdabd560d034312db8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test a question generation inference"
      ],
      "metadata": {
        "id": "4jDDSWB15dqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pipelines import pipeline\n",
        "import textwrap\n",
        "import datetime\n",
        "\n",
        "nlp = pipeline(\"e2e-qg\", model=\"/content/t5-small-e2e-qg\", tokenizer=\"/content/t5-small-e2e-qg\")\n",
        "\n",
        "ques = nlp(\"Python is a programming language. Created by Guido van Rossum and first released in 1991.\")\n",
        "\n",
        "print(ques)\n",
        "print(\"---\")\n",
        "text2 =  \"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.\"\n",
        "ques2 = nlp(text2)\n",
        "print(ques2)\n",
        "print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bySba8kr32xH",
        "outputId": "6fc605ac-e2dc-45d7-afdf-dca0c45c4bf7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What is a programming language?', 'Who created Python?', 'When was Python first released?']\n",
            "---\n",
            "['What do diffusion models achieve by decomposing the image formation process into a sequential application of denoising autoencoders?', 'What is a guiding mechanism to control the image generation process without retraining?', 'How long does optimization of powerful DMs typically consume?']\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##¬† Inference for LONG text"
      ],
      "metadata": {
        "id": "GgbnYPpZgSs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fname = '/content/BERTexplanation.txt'\n",
        "with open(fname) as f:\n",
        "    doc = f.read()\n",
        "f.close()"
      ],
      "metadata": {
        "id": "daZx0mfrgTVr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of characters\n",
        "len(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6D_11-lxqy2",
        "outputId": "cfaed810-e51e-4284-afa6-8fa16e722fef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11032"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of words\n",
        "len(doc.split(' '))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5dG0nZAxtu1",
        "outputId": "6e7a1abc-d83d-4e02-f03b-853a300eacf6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1734"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mysplit(text,chunk,overlap):\n",
        "  from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "        # Set a really small chunk size, just to show.\n",
        "        chunk_size = chunk,\n",
        "        chunk_overlap  = overlap,\n",
        "        length_function = len,\n",
        "        )\n",
        "  texts = text_splitter.split_text(text)\n",
        "  return texts"
      ],
      "metadata": {
        "id": "oiwyF-fre3Gk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = mysplit(doc,6000,150)\n",
        "for test in texts:\n",
        "  print(\"---\")\n",
        "  questions = nlp(test)\n",
        "  for i in questions:\n",
        "    print('- '+i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhI3ZcTvgp3B",
        "outputId": "d7dd5c76-f034-4b0c-b2df-866ac9ed83a2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "- What is the name of the book written by Pushpam Punjabi author?\n",
            "- What is a key component of NLP?\n",
            "- How does a machine understand human language?\n",
            "- Who developed Understanding BERT BERT?\n",
            "---\n",
            "- What is a useful technique for a variety of NLP tasks?\n",
            "- How can BERT better understand the overall meaning and context of a passage of text?\n",
            "- What is important for applications such as chatbots or virtual assistants where the ability to understand and interpret human language is crucial for providing accurate and helpful responses?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = mysplit(doc,3700,50)\n",
        "doma = []\n",
        "for test in texts:\n",
        "  print(\"---\")\n",
        "  questions = nlp(test)\n",
        "  for i in questions:\n",
        "    doma.append(i)\n",
        "    print('- '+i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpeBZmGehVQt",
        "outputId": "f621becc-5cb4-4f67-fce3-6092e8454483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "- What is the name of the book written by Pushpam Punjabi author?\n",
            "- What is a key component of NLP?\n",
            "- How does a machine understand human language?\n",
            "- Who developed Understanding BERT BERT?\n",
            "---\n",
            "- How many datasets does BERT use?\n",
            "- What is the name of the dataset that BERT is trained on?\n",
            "- How many words are masked in BERT?\n",
            "---\n",
            "- What is a powerful tool for a variety of NLP applications?\n",
            "- What does fine-tuning a BERT model enable researchers and developers to achieve higher levels of accuracy and performance on specific tasks?\n",
            "- BERT uses a unique ‚Äútransformer‚Äù architecture that enables it to better understand the context and meaning of words and phrases in a sentence?\n",
            "---\n",
            "- Who is Pushpam Punjabi?\n",
            "- What is the name of the machine learning engineer who develops solutions for the use cases emerging in the field of NLP/Natural Language Understanding?\n"
          ]
        }
      ]
    }
  ]
}